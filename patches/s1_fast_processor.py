#!/usr/bin/env python3
"""
s1_fast_processor.py â€” Sentinel-1 RTC å¿«é€Ÿä¸‹è½½ & ROI æ‹¼æ¥ (çµæ´»å¹¶è¡Œå¤„ç†ç‰ˆæœ¬)
æ›´æ–°ï¼š2025-05-20
æ”¯æŒçµæ´»çš„å¹¶è¡Œåˆ†åŒºå¤„ç†ï¼ŒåŒ…å«å®Œå–„çš„é”™è¯¯å¤„ç†å’Œè¶…æ—¶æ§åˆ¶
"""

from __future__ import annotations
import os, sys, argparse, logging, datetime, time, warnings, signal
from pathlib import Path
from collections import defaultdict
from contextlib import contextmanager

import numpy as np
import psutil, rasterio, xarray as xr, rioxarray
from rasterio.enums import Resampling
from rasterio.warp import transform_bounds, reproject
from rasterio.merge import merge
import pystac_client, planetary_computer, stackstac
import shapely.geometry
import concurrent.futures
import uuid
import tempfile
import shutil

import dask
from dask.distributed import Client, LocalCluster, performance_report, wait

# â–¶ distributed ç‰ˆæœ¬å…¼å®¹
try:
    from distributed.comm.core import CommClosedError
except ImportError:
    from distributed import CommClosedError

warnings.filterwarnings("ignore", category=RuntimeWarning, module="dask.core")
warnings.filterwarnings("ignore", category=rasterio.errors.NotGeoreferencedWarning)
warnings.filterwarnings("ignore", category=UserWarning, message=".*The array is being split into many small chunks.*")
warnings.filterwarnings("ignore", message=".*invalid value encountered in true_divide.*")
warnings.filterwarnings("ignore", message=".*invalid value encountered in log10.*")

# â”€â”€â”€ å¸¸é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Sentinel-1 åˆ†è¾¨ç‡ (ç±³)
SAR_RESOLUTION = 10.0

# æœ‰æ•ˆè¦†ç›–ç‡é˜ˆå€¼ (ä½äºæ­¤å€¼è·³è¿‡å¤„ç†)
MIN_VALID_COVERAGE = 10.0  # ç™¾åˆ†æ¯”

# è¶…æ—¶è®¾ç½®ï¼ˆç§’ï¼‰
PROCESS_TIMEOUT = 120 * 60  # æ€»ä½“è¶…æ—¶
DAY_TIMEOUT = 40 * 60      # å•æ—¥å¤„ç†è¶…æ—¶
ITEM_TIMEOUT = 20 * 60      # å•ä¸ªitemå¤„ç†è¶…æ—¶

# â”€â”€â”€ è¶…æ—¶æ§åˆ¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TimeoutException(Exception):
    pass

@contextmanager
def timeout_handler(seconds):
    """è¶…æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    def timeout_signal_handler(signum, frame):
        raise TimeoutException(f"æ“ä½œè¶…æ—¶ ({seconds}ç§’)")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨ä¸»çº¿ç¨‹ä¸­ï¼ˆUnixä¿¡å·åªèƒ½åœ¨ä¸»çº¿ç¨‹ä¸­å¤„ç†ï¼‰
    import threading
    if threading.current_thread() is not threading.main_thread():
        # å¦‚æœä¸æ˜¯ä¸»çº¿ç¨‹ï¼Œåªæ˜¯yieldè€Œä¸è®¾ç½®ä¿¡å·
        yield
        return
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    old_handler = signal.signal(signal.SIGALRM, timeout_signal_handler)
    signal.alarm(seconds)
    
    try:
        yield
    finally:
        # æ¢å¤åŸä¿¡å·å¤„ç†å™¨
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

# â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_args():
    P = argparse.ArgumentParser("Fast Sentinel-1 RTC Processor (Flexible Parallel Edition)")
    P.add_argument("--input_tiff",   required=True)
    P.add_argument("--start_date",   required=True)
    P.add_argument("--end_date",     required=True)
    P.add_argument("--output",       default="sentinel1_output")
    P.add_argument("--orbit_state",  default="both", choices=["ascending", "descending", "both"])
    P.add_argument("--dask_workers", type=int,   default=8)
    P.add_argument("--worker_memory",type=int,   default=16)
    P.add_argument("--chunksize",    type=int,   default=1024)
    P.add_argument("--workers",      type=int,   default=8)
    P.add_argument("--overwrite",    action="store_true")
    P.add_argument("--debug",        action="store_true")
    P.add_argument("--min_coverage", type=float, default=MIN_VALID_COVERAGE,
                   help="æœ€å°æœ‰æ•ˆåƒç´ è¦†ç›–ç‡ (ç™¾åˆ†æ¯”)")
    P.add_argument("--partition_id", default="unknown",
                   help="åˆ†åŒºIDï¼ˆç”¨äºæ—¥å¿—æ ‡è¯†ï¼‰")
    return P.parse_args()

# â”€â”€â”€ logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def setup_logging(debug: bool, out_dir: Path, partition_id: str):
    """è®¾ç½®æ—¥å¿—ï¼ŒåŒ…å«åˆ†åŒºIDæ ‡è¯†"""
    fmt = f"%(asctime)s [{partition_id}] [%(levelname)s] %(message)s"
    lvl = logging.DEBUG if debug else logging.INFO
    
    # åˆ›å»ºlogger
    logger = logging.getLogger()
    logger.setLevel(lvl)
    
    # æ¸…é™¤ç°æœ‰çš„handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # åˆ›å»ºformatter
    formatter = logging.Formatter(fmt)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆåˆ†åŒºç‰¹å®šçš„æ—¥å¿—æ–‡ä»¶ï¼‰
    file_handler = logging.FileHandler(
        out_dir / f"s1_{partition_id}_detail.log", 
        "a", 
        encoding="utf-8"
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

def log_sys(partition_id: str):
    m = psutil.virtual_memory()
    logging.info(f"[{partition_id}] ç³»ç»Ÿä¿¡æ¯ - CPU {os.cpu_count()} | "
                 f"RAM {m.total/1e9:.1f} GB (free {m.available/1e9:.1f} GB)")

def fmt_bbox(b):
    return f"{b[0]:.5f},{b[1]:.5f} â‡¢ {b[2]:.5f},{b[3]:.5f}"

# â”€â”€â”€ Dask â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_client(req_workers:int, req_mem:int, partition_id: str):
    """åˆ›å»ºDaskå®¢æˆ·ç«¯ï¼Œä½¿ç”¨åˆ†åŒºç‰¹å®šçš„dashboardç«¯å£"""
    total_mem = psutil.virtual_memory().total / 1e9
    workers = min(req_workers, os.cpu_count(),
                  max(1, int(total_mem // (req_mem*1.2))))
    if workers < req_workers:
        logging.warning(f"âš ï¸  worker æ•° {req_workers}â†’{workers} (èµ„æºé™åˆ¶)")
    
    # ä¸ºä¸åŒåˆ†åŒºè‡ªåŠ¨åˆ†é…dashboardç«¯å£
    # ä½¿ç”¨åˆ†åŒºIDçš„å“ˆå¸Œå€¼æ¥ç¡®å®šç«¯å£ï¼Œç¡®ä¿ç›¸åŒIDæ€»æ˜¯ä½¿ç”¨ç›¸åŒç«¯å£
    # å°†ç«¯å£é™åˆ¶åœ¨8700-8779ä¹‹é—´
    port_base = 8700
    port_range = 80
    dashboard_port = port_base + (hash(partition_id) % port_range)
    
    cluster = LocalCluster(
        n_workers         = workers,
        threads_per_worker= 4,
        processes         = True,
        memory_limit      = f"{req_mem}GB",
        dashboard_address = f":{dashboard_port}",
        silence_logs      = "ERROR",
    )
    dask.config.set({
        "distributed.worker.memory.target": 0.80,
        "distributed.worker.memory.spill":  0.90,
        "distributed.worker.memory.pause":  0.95,
    })
    cli = Client(cluster, asynchronous=False)
    logging.info(f"[{partition_id}] Dask dashboard â†’ {cli.dashboard_link}")
    return cli

# â”€â”€â”€ ROI & æ©è†œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_roi(tiff: Path, partition_id: str):
    with rasterio.open(tiff) as src:
        tpl = dict(crs=src.crs,
                   transform=src.transform,
                   width=src.width,
                   height=src.height)
        bbox_proj = src.bounds
        bbox_ll   = transform_bounds(src.crs, "EPSG:4326", *bbox_proj,
                                     densify_pts=21)
        mask_np   = (src.read(1) > 0).astype(np.uint8)
    logging.info(f"[{partition_id}] ROI (CRS={tpl['crs']}): {tpl['width']}Ã—{tpl['height']}")
    logging.info(f"[{partition_id}] ROI bbox proj: {fmt_bbox(bbox_proj)}")
    logging.info(f"[{partition_id}] ROI bbox lon/lat: {fmt_bbox(bbox_ll)}")
    return tpl, bbox_proj, bbox_ll, mask_np

def mask_to_xr(mask_np, tpl):
    da = xr.DataArray(mask_np, dims=("y", "x"))
    return da.rio.write_crs(tpl["crs"]).rio.write_transform(tpl["transform"])

# â”€â”€â”€ STAC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_items(bbox_ll, date_range:str, orbit_state="both", partition_id="unknown"):
    cat = pystac_client.Client.open(
        "https://planetarycomputer.microsoft.com/api/stac/v1",
        modifier=planetary_computer.sign_inplace)
    
    query = {"collections": ["sentinel-1-rtc"], "bbox": bbox_ll, "datetime": date_range}
    
    # å¦‚æœæŒ‡å®šäº†è½¨é“æ–¹å‘ï¼Œæ·»åŠ è¿‡æ»¤æ¡ä»¶
    if orbit_state != "both":
        query["query"] = {"sat:orbit_state": {"eq": orbit_state}}
    
    q = cat.search(**query)
    items = list(q.get_items())
    logging.info(f"[{partition_id}] STAC å‘½ä¸­ {len(items)} item")
    if items:
        b = np.array([it.bbox for it in items])
        union = [b[:,0].min(), b[:,1].min(), b[:,2].max(), b[:,3].max()]
        logging.info(f"[{partition_id}] All item union lon/lat: {fmt_bbox(union)}")
    return items

def group_by_date_orbit(items, partition_id: str):
    g = defaultdict(list)
    for it in items:
        d = it.properties["datetime"][:10]
        orbit = it.properties.get("sat:orbit_state", "unknown")
        key = f"{d}_{orbit}"
        g[key].append(it)
    logging.info(f"[{partition_id}] â‡’ {len(g)} è§‚æµ‹æ—¥-è½¨é“ç»„åˆ")
    return dict(sorted(g.items()))

# â”€â”€â”€ æŒ¯å¹…è½¬dB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def amplitude_to_db(amp, mask=None):
    """
    æŒ¯å¹…è½¬æ¢ä¸ºdBå€¼ (å¸¦åç§»å’Œç¼©æ”¾ä»¥é€‚é…int16å­˜å‚¨)
    
    è½¬æ¢å…¬å¼:
    dB = 20 * log10(amp)
    shifted = dB + 50     # åç§»é¿å…è´Ÿå€¼
    scaled = shifted * 200 # ç¼©æ”¾ä¿ç•™ç²¾åº¦
    clipped = np.clip(scaled, 0, 30000) # è£å‰ªåˆ°int16èŒƒå›´å†…
    """
    # ç¡®ä¿ampæ˜¯numpyæ•°ç»„
    if hasattr(amp, 'values'):
        amp_array = amp.values
    elif hasattr(amp, 'compute'):
        amp_array = amp.compute()
    else:
        amp_array = np.asarray(amp)
    
    # åˆ›å»ºè¾“å‡ºæ•°ç»„
    output = np.zeros_like(amp_array, dtype=np.int16)
    
    # å®‰å…¨å¤„ç†å¯èƒ½çš„æ— æ•ˆå€¼
    with np.errstate(invalid='ignore', divide='ignore'):
        # ç¡®ä¿ampæ˜¯æœ‰é™çš„(éNaNæˆ–inf)
        amp_finite = np.isfinite(amp_array)
        # åˆ›å»ºæœ‰æ•ˆå€¼æ©è†œ (> 0)
        valid_mask = amp_finite & (amp_array > 0)
    
    # ä»…å¤„ç†æœ‰æ•ˆåƒç´ 
    if np.any(valid_mask):
        # ä»…è®¡ç®—æœ‰æ•ˆåƒç´ çš„dBå€¼
        with np.errstate(invalid='ignore', divide='ignore'):
            # ç›´æ¥å¯¹æœ‰æ•ˆä½ç½®è¿›è¡Œè®¡ç®—ï¼Œé¿å…å¸ƒå°”ç´¢å¼•é—®é¢˜
            valid_indices = np.where(valid_mask)
            valid_amp = amp_array[valid_indices]
            
            db = 20.0 * np.log10(valid_amp)
            db_shift = db + 50.0
            scaled = db_shift * 200.0
            clipped = np.clip(scaled, 0, 32767)  # æˆªæ–­åˆ°int16èŒƒå›´
        
        # èµ‹å€¼åˆ°è¾“å‡ºæ•°ç»„
        output[valid_indices] = clipped.astype(np.int16)
    
    # åº”ç”¨å¤–éƒ¨æ©è†œ
    if mask is not None:
        # ç¡®ä¿maskæ˜¯numpyæ•°ç»„
        if hasattr(mask, 'values'):
            mask_array = mask.values
        else:
            mask_array = np.asarray(mask)
        
        # å¤„ç†å½¢çŠ¶ä¸åŒ¹é…çš„æƒ…å†µ
        if output.shape != mask_array.shape:
            # è®¡ç®—å…±åŒåŒºåŸŸ
            common_shape = tuple(min(output.shape[i], mask_array.shape[i]) for i in range(len(output.shape)))
            
            # åˆ›å»ºè£å‰ªåçš„æ•°ç»„
            if len(common_shape) == 2:
                output_cropped = output[:common_shape[0], :common_shape[1]]
                mask_cropped = mask_array[:common_shape[0], :common_shape[1]]
                output[:common_shape[0], :common_shape[1]] = np.where(mask_cropped > 0, output_cropped, 0)
                # æ¸…é›¶è¶…å‡ºmaskèŒƒå›´çš„åŒºåŸŸ
                if output.shape[0] > common_shape[0]:
                    output[common_shape[0]:, :] = 0
                if output.shape[1] > common_shape[1]:
                    output[:, common_shape[1]:] = 0
            else:
                # å¦‚æœä¸æ˜¯2Dï¼Œä½¿ç”¨ç®€å•çš„è£å‰ª
                output = np.where(mask_array > 0, output, 0)
        else:
            output = np.where(mask_array > 0, output, 0)
        
    return output

# â”€â”€â”€ GeoTIFF å†™å‡º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_tiff(np_arr, out_path: Path, tpl, dtype, metadata=None):
    # å¤„ç†NaNå€¼
    if np.isnan(np_arr).any():
        np_arr = np.nan_to_num(np_arr, nan=0)
        
    profile = dict(driver="GTiff", dtype=dtype, count=1,
                   width=tpl["width"], height=tpl["height"],
                   crs=tpl["crs"], transform=tpl["transform"],
                   compress="lzw", tiled=True,
                   blockxsize=256, blockysize=256,
                   nodata=0)
    with rasterio.open(out_path, "w", **profile) as dst:
        dst.write(np_arr.astype(dtype, copy=False), 1)
        
        # è®¾ç½®æ³¢æ®µæè¿°å’Œå…ƒæ•°æ®
        if metadata:
            dst.set_band_description(1, metadata.get("band_desc", ""))
            dst.update_tags(**metadata)

# â”€â”€â”€ éªŒè¯TIFF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_tiff(file_path, expected_shape, expected_crs, expected_transform):
    """éªŒè¯TIFFæ–‡ä»¶æ˜¯å¦å…·æœ‰é¢„æœŸçš„å±æ€§"""
    try:
        with rasterio.open(file_path) as src:
            # æ£€æŸ¥åŸºæœ¬å±æ€§
            if src.shape != expected_shape:
                logging.warning(f"éªŒè¯å¤±è´¥: {file_path} å½¢çŠ¶ä¸åŒ¹é…. é¢„æœŸ {expected_shape}, å¾—åˆ° {src.shape}")
                return False
            
            if src.crs != expected_crs:
                logging.warning(f"éªŒè¯å¤±è´¥: {file_path} CRSä¸åŒ¹é…. é¢„æœŸ {expected_crs}, å¾—åˆ° {src.crs}")
                return False
            
            # æ£€æŸ¥è½¬æ¢çŸ©é˜µ
            if not np.allclose(np.array(src.transform)[:6], np.array(expected_transform)[:6], rtol=1e-05, atol=1e-08):
                logging.warning(f"éªŒè¯å¤±è´¥: {file_path} è½¬æ¢çŸ©é˜µä¸åŒ¹é….")
                return False
            
            # æ£€æŸ¥æ•°æ®å­˜åœ¨æ€§
            stats = [src.statistics(i) for i in range(1, src.count + 1)]
            if any(s.max == 0 and s.min == 0 for s in stats):
                logging.warning(f"éªŒè¯å¤±è´¥: {file_path} æ³¢æ®µå…¨ä¸ºé›¶")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            expected_size_mb = (src.width * src.height * src.count * 2) / (1024 * 1024)  # int16 = 2 bytes
            if file_size_mb < expected_size_mb * 0.05:  # è€ƒè™‘å‹ç¼©ç‡ï¼Œä½†ä¸èƒ½å¤ªå°
                logging.warning(f"éªŒè¯å¤±è´¥: {file_path} æ–‡ä»¶å¤ªå°. é¢„æœŸ ~{expected_size_mb:.2f}MB, å¾—åˆ° {file_size_mb:.2f}MB")
                return False
            
            logging.debug(f"TIFFéªŒè¯é€šè¿‡: {file_path}ï¼Œå½¢çŠ¶={src.shape}, å¤§å°={file_size_mb:.2f}MB")
            return True
            
    except Exception as e:
        logging.error(f"éªŒè¯TIFF {file_path} æ—¶å‡ºé”™: {e}")
        return False

# â”€â”€â”€ è¦†ç›–ç‡åˆ†æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_coverage(data_arr, roi_mask, partition_id: str):
    """åˆ†ææ•°æ®è¦†ç›–ROIçš„æƒ…å†µï¼Œå¤„ç†å½¢çŠ¶ä¸åŒ¹é…çš„æƒ…å†µ"""
    # ç¡®ä¿data_arræ˜¯numpyæ•°ç»„
    if hasattr(data_arr, 'values'):
        data_values = data_arr.values
    elif hasattr(data_arr, 'compute'):
        data_values = data_arr.compute()
    else:
        data_values = np.asarray(data_arr)
    
    # åˆ›å»ºæœ‰æ•ˆå€¼æ©è†œ (éé›¶ä¸”æœ‰é™)ï¼ŒæŠ‘åˆ¶æ— æ•ˆå€¼è­¦å‘Š
    with np.errstate(invalid='ignore'):
        valid_mask = (data_values > 0) & np.isfinite(data_values)
    
    # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…ï¼Œå¦‚ä¸åŒ¹é…åˆ™è°ƒæ•´
    if len(valid_mask.shape) == 2 and valid_mask.shape != roi_mask.shape:
        logging.info(f"[{partition_id}]     æ•°æ®å½¢çŠ¶ {valid_mask.shape} ä¸ROIå½¢çŠ¶ {roi_mask.shape} ä¸åŒ¹é…ï¼Œè£å‰ªåˆ°å…±åŒåŒºåŸŸ")
        # è®¡ç®—å…±åŒçš„é«˜åº¦å’Œå®½åº¦
        common_height = min(valid_mask.shape[0], roi_mask.shape[0])
        common_width = min(valid_mask.shape[1], roi_mask.shape[1])
        
        # è£å‰ªä¸¤ä¸ªæ•°ç»„åˆ°å…±åŒåŒºåŸŸ
        valid_mask_cropped = valid_mask[:common_height, :common_width]
        roi_mask_cropped = roi_mask[:common_height, :common_width]
        
        # ä½¿ç”¨è£å‰ªåçš„æ©è†œç»§ç»­åˆ†æï¼Œç¡®ä¿è¿”å›æ•°å€¼
        valid_count = int(np.sum(valid_mask_cropped & roi_mask_cropped))
        roi_count = int(np.sum(roi_mask_cropped))
        valid_pct = 100 * valid_count / roi_count if roi_count > 0 else 0
        logging.info(f"[{partition_id}]     å•ä¸ªTile: ROIå†…æœ‰æ•ˆåƒç´  {valid_count}/{roi_count} ({valid_pct:.2f}%)")
        
        return valid_mask, valid_pct
    
    # å¤„ç†3Dæ•°ç»„
    elif len(valid_mask.shape) == 3:
        # å¤šä¸ªtileæƒ…å†µ
        n_tiles = valid_mask.shape[0]
        tile_stats = []
        
        for i in range(n_tiles):
            # æ£€æŸ¥å½“å‰tileçš„å½¢çŠ¶æ˜¯å¦åŒ¹é…ROI
            if valid_mask[i].shape != roi_mask.shape:
                logging.info(f"[{partition_id}]     Tile {i} å½¢çŠ¶ {valid_mask[i].shape} ä¸ROIå½¢çŠ¶ {roi_mask.shape} ä¸åŒ¹é…ï¼Œè£å‰ªåˆ°å…±åŒåŒºåŸŸ")
                # è®¡ç®—å…±åŒåŒºåŸŸ
                common_height = min(valid_mask[i].shape[0], roi_mask.shape[0])
                common_width = min(valid_mask[i].shape[1], roi_mask.shape[1])
                
                # è£å‰ªå½“å‰tileå’ŒROIæ©è†œ
                tile_valid = valid_mask[i][:common_height, :common_width]
                roi_cropped = roi_mask[:common_height, :common_width]
            else:
                tile_valid = valid_mask[i]
                roi_cropped = roi_mask
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®ï¼Œç¡®ä¿è¿”å›æ•°å€¼
            valid_count = int(np.sum(tile_valid & roi_cropped))
            roi_count = int(np.sum(roi_cropped))
            valid_pct = 100 * valid_count / roi_count if roi_count > 0 else 0
            logging.info(f"[{partition_id}]     Tile {i}: ROIå†…æœ‰æ•ˆåƒç´  {valid_count}/{roi_count} ({valid_pct:.2f}%)")
            tile_stats.append(valid_pct)
        
        # å–æœ€å¤§è¦†ç›–ç‡ä½œä¸ºæ€»è¦†ç›–ç‡ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        if tile_stats:
            total_valid_pct = max(tile_stats)
            logging.info(f"[{partition_id}]     åˆå¹¶å: ROIå†…æœ‰æ•ˆåƒç´ è¦†ç›–ç‡ {total_valid_pct:.2f}%")
        else:
            total_valid_pct = 0
            logging.info(f"[{partition_id}]     æ— æœ‰æ•ˆè¦†ç›–")
        
        return valid_mask, total_valid_pct
    
    else:
        # å•tileæƒ…å†µï¼Œå½¢çŠ¶åŒ¹é…
        tile_valid = valid_mask & roi_mask
        valid_count = int(np.sum(tile_valid))
        roi_count = int(np.sum(roi_mask))
        valid_pct = 100 * valid_count / roi_count if roi_count > 0 else 0
        logging.info(f"[{partition_id}]     å•ä¸ªTile: ROIå†…æœ‰æ•ˆåƒç´  {valid_count}/{roi_count} ({valid_pct:.2f}%)")
        
        return valid_mask, valid_pct

# â”€â”€â”€ å¤„ç†å•ä¸ªitem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_item(item, tpl, bbox_proj, mask_np, resolution, chunksize, temp_dir, min_coverage, partition_id, retries=2):
    """å¤„ç†å•ä¸ªSentinel-1 itemå¹¶ä¿å­˜ä¸ºVV/VHä¸¤ä¸ªTIFFæ–‡ä»¶ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
    
    Returns:
        tuple: (vv_path, vh_path, status) 
        status: "success", "skipped", "failed"
    """
    orbit_state = item.properties.get("sat:orbit_state", "unknown")
    date_str = item.properties.get("datetime").split("T")[0]
    item_id = item.id
    
    # ç¡®ä¿ä¸´æ—¶ç›®å½•æ˜¯Pathå¯¹è±¡
    temp_dir = Path(temp_dir)
    
    # ç”Ÿæˆå”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶å
    uid = uuid.uuid4().hex[:8]
    vv_temp = temp_dir / f"{date_str}_vv_{orbit_state}_{uid}.tiff"
    vh_temp = temp_dir / f"{date_str}_vh_{orbit_state}_{uid}.tiff"
    
    logging.info(f"[{partition_id}]   å¤„ç†item {item_id} ({date_str}_{orbit_state})")
    
    for attempt in range(retries + 1):
        try:
            # ä½¿ç”¨è¶…æ—¶æ§åˆ¶å¤„ç†å•ä¸ªitem
            with timeout_handler(ITEM_TIMEOUT):
                # ä½¿ç”¨stackstacåŠ è½½æ•°æ®
                ds = stackstac.stack(
                    [item], 
                    bounds=bbox_proj,
                    epsg=tpl["crs"].to_epsg(),
                    resolution=resolution,
                    chunksize=chunksize
                )
                
                # æ£€æŸ¥æ³¢æ®µæ˜¯å¦å­˜åœ¨
                if 'vv' not in ds.band.values or 'vh' not in ds.band.values:
                    logging.warning(f"[{partition_id}]   {date_str}_{orbit_state} ç¼ºå°‘å¿…éœ€çš„æ³¢æ®µï¼Œè·³è¿‡")
                    return None, None, "skipped"
                    
                # æå–VVå’ŒVHæ•°æ®
                vv_data = ds.sel(band="vv").squeeze()
                vh_data = ds.sel(band="vh").squeeze()
                
                # è®¡ç®—æ•°æ®ï¼ˆè§¦å‘å®é™…çš„æ•°æ®åŠ è½½ï¼‰
                try:
                    # æ˜¾å¼è®¡ç®—æ•°æ®ï¼Œè¿™ä¼šè§¦å‘å®é™…çš„è¿œç¨‹è¯»å–
                    vv_values = vv_data.compute()
                    vh_values = vh_data.compute()
                except Exception as compute_error:
                    if attempt < retries:
                        logging.warning(f"[{partition_id}]   å°è¯• {attempt+1}/{retries+1} è®¡ç®—æ•°æ®å¤±è´¥: {compute_error}ï¼Œé‡è¯•...")
                        time.sleep(2)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
                        continue
                    else:
                        raise
                
                # è·å–æ•°æ®çš„å®é™…å°ºå¯¸
                vv_shape = vv_values.shape
                logging.debug(f"[{partition_id}]   VVæ•°æ®å½¢çŠ¶: {vv_shape}, ROIå½¢çŠ¶: {mask_np.shape}")
                
                # åˆ†æè¦†ç›–ç‡
                logging.info(f"[{partition_id}]   åˆ†æVVæ³¢æ®µè¦†ç›–ç‡")
                vv_valid_mask, vv_valid_pct = analyze_coverage(vv_values, mask_np, partition_id)
                
                logging.info(f"[{partition_id}]   åˆ†æVHæ³¢æ®µè¦†ç›–ç‡")
                vh_valid_mask, vh_valid_pct = analyze_coverage(vh_values, mask_np, partition_id)
                
                # æ£€æŸ¥è¦†ç›–ç‡æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                if vv_valid_pct < min_coverage and vh_valid_pct < min_coverage:
                    logging.warning(f"[{partition_id}]   âš ï¸ {date_str}_{orbit_state} æœ‰æ•ˆè¦†ç›–ç‡ VV={vv_valid_pct:.2f}%, VH={vh_valid_pct:.2f}% å‡ä½äº {min_coverage}%ï¼Œè·³è¿‡")
                    return None, None, "skipped"
                
                # å¤„ç†VVæ•°æ®: æŒ¯å¹…è½¬dBå¹¶åº”ç”¨ROIæ©è†œ
                if vv_valid_pct >= min_coverage:
                    logging.info(f"[{partition_id}]   å¤„ç†VVæ³¢æ®µ")
                    
                    # ç¡®ä¿æ©è†œå½¢çŠ¶åŒ¹é…
                    common_height = min(vv_values.shape[0], mask_np.shape[0])
                    common_width = min(vv_values.shape[1], mask_np.shape[1])
                    
                    # è£å‰ªæ•°æ®å’Œæ©è†œ
                    vv_cropped = vv_values[:common_height, :common_width]
                    mask_cropped = mask_np[:common_height, :common_width]
                    
                    # åº”ç”¨æŒ¯å¹…è½¬dBå¤„ç†
                    vv_db = amplitude_to_db(vv_cropped, mask=mask_cropped)
                    
                    # å‡†å¤‡å®Œæ•´å°ºå¯¸çš„è¾“å‡ºæ•°ç»„
                    vv_final = np.zeros((tpl["height"], tpl["width"]), dtype=np.int16)
                    vv_final[:common_height, :common_width] = vv_db
                    
                    # å†™å‡ºVV TIFF
                    vv_metadata = {
                        "band_desc": "VV polarization, amplitude to dB, +50 offset, scale=200",
                        "TIFFTAG_DATETIME": datetime.datetime.now().strftime("%Y:%m:%d %H:%M:%S"),
                        "ORBIT_STATE": orbit_state,
                        "DATE_ACQUIRED": date_str,
                        "POLARIZATION": "VV",
                        "DESCRIPTION": "Sentinel-1 SAR data (VV). Values are amplitude converted to dB, shifted by +50, scaled by 200."
                    }
                    write_tiff(vv_final, vv_temp, tpl, "int16", vv_metadata)
                    
                    # éªŒè¯VVè¾“å‡º
                    if not validate_tiff(vv_temp, (tpl["height"], tpl["width"]), tpl["crs"], tpl["transform"]):
                        logging.error(f"[{partition_id}]   âœ— VVè¾“å‡ºéªŒè¯å¤±è´¥")
                        if vv_temp.exists():
                            vv_temp.unlink()
                        vv_temp = None
                    else:
                        logging.info(f"[{partition_id}]   âœ“ VV: {os.path.getsize(vv_temp)/1e6:.2f} MB")
                else:
                    logging.warning(f"[{partition_id}]   âš ï¸ VVè¦†ç›–ç‡ä¸è¶³ï¼Œè·³è¿‡")
                    vv_temp = None
                
                # å¤„ç†VHæ•°æ®: æŒ¯å¹…è½¬dBå¹¶åº”ç”¨ROIæ©è†œ
                if vh_valid_pct >= min_coverage:
                    logging.info(f"[{partition_id}]   å¤„ç†VHæ³¢æ®µ")
                    
                    # ç¡®ä¿æ©è†œå½¢çŠ¶åŒ¹é…
                    common_height = min(vh_values.shape[0], mask_np.shape[0])
                    common_width = min(vh_values.shape[1], mask_np.shape[1])
                    
                    # è£å‰ªæ•°æ®å’Œæ©è†œ
                    vh_cropped = vh_values[:common_height, :common_width]
                    mask_cropped = mask_np[:common_height, :common_width]
                    
                    # åº”ç”¨æŒ¯å¹…è½¬dBå¤„ç†
                    vh_db = amplitude_to_db(vh_cropped, mask=mask_cropped)
                    
                    # å‡†å¤‡å®Œæ•´å°ºå¯¸çš„è¾“å‡ºæ•°ç»„
                    vh_final = np.zeros((tpl["height"], tpl["width"]), dtype=np.int16)
                    vh_final[:common_height, :common_width] = vh_db
                    
                    # å†™å‡ºVH TIFF
                    vh_metadata = {
                        "band_desc": "VH polarization, amplitude to dB, +50 offset, scale=200",
                        "TIFFTAG_DATETIME": datetime.datetime.now().strftime("%Y:%m:%d %H:%M:%S"),
                        "ORBIT_STATE": orbit_state,
                        "DATE_ACQUIRED": date_str,
                        "POLARIZATION": "VH",
                        "DESCRIPTION": "Sentinel-1 SAR data (VH). Values are amplitude converted to dB, shifted by +50, scaled by 200."
                    }
                    write_tiff(vh_final, vh_temp, tpl, "int16", vh_metadata)
                    
                    # éªŒè¯VHè¾“å‡º
                    if not validate_tiff(vh_temp, (tpl["height"], tpl["width"]), tpl["crs"], tpl["transform"]):
                        logging.error(f"[{partition_id}]   âœ— VHè¾“å‡ºéªŒè¯å¤±è´¥")
                        if vh_temp.exists():
                            vh_temp.unlink()
                        vh_temp = None
                    else:
                        logging.info(f"[{partition_id}]   âœ“ VH: {os.path.getsize(vh_temp)/1e6:.2f} MB")
                else:
                    logging.warning(f"[{partition_id}]   âš ï¸ VHè¦†ç›–ç‡ä¸è¶³ï¼Œè·³è¿‡")
                    vh_temp = None
                
                # æ£€æŸ¥æœ€ç»ˆçŠ¶æ€
                if vv_temp or vh_temp:
                    # æˆåŠŸå®Œæˆï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    return vv_temp, vh_temp, "success"
                else:
                    # æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡ä»¶ï¼Œä½†è¿™æ˜¯è¦†ç›–ç‡ä¸è¶³å¯¼è‡´çš„ï¼Œç®—ä½œè·³è¿‡
                    return None, None, "skipped"
                
        except TimeoutException as e:
            if attempt < retries:
                logging.warning(f"[{partition_id}]   å°è¯• {attempt+1}/{retries+1} å¤„ç† {date_str}_{orbit_state} è¶…æ—¶ï¼Œé‡è¯•...")
                time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
                continue
            else:
                logging.error(f"[{partition_id}]   âš ï¸ å¤„ç† {date_str}_{orbit_state} è¶…æ—¶: {e}")
                return None, None, "failed"
        except (RuntimeError, Exception) as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œ/IOé”™è¯¯
            error_msg = str(e).lower()
            is_retriable_error = any(keyword in error_msg for keyword in [
                'rasterio', 'read', 'tiff', 'network', 'timeout', 'connection', 'io'
            ])
            
            if is_retriable_error and attempt < retries:
                logging.warning(f"[{partition_id}]   å°è¯• {attempt+1}/{retries+1} å¤„ç† {date_str}_{orbit_state} å‡ºé”™: {type(e).__name__} - {e}ï¼Œé‡è¯•...")
                time.sleep(3)  # ç­‰å¾…3ç§’åé‡è¯•
                continue
            else:
                logging.error(f"[{partition_id}]   âœ— å¤„ç† {date_str}_{orbit_state} å‡ºé”™: {type(e).__name__} - {e}")
                return None, None, "failed"
    
    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
    logging.error(f"[{partition_id}]   âœ— å¤„ç† {date_str}_{orbit_state} å…¨éƒ¨é‡è¯•å¤±è´¥")
    return None, None, "failed"

# â”€â”€â”€ é•¶åµŒå¤šä¸ªTIFF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def mosaic_tiffs(tiff_paths, output_path, tpl, date_str, orbit_state, polarization, partition_id):
    """é•¶åµŒå¤šä¸ªTIFFä¸ºä¸€ä¸ªè¾“å‡ºTIFF"""
    try:
        # ç¡®ä¿è¾“å‡ºè·¯å¾„æ˜¯Pathå¯¹è±¡
        output_path = Path(output_path)
        
        # æ‰“å¼€æ‰€æœ‰æºTIFF
        src_files = []
        for path in tiff_paths:
            if path and os.path.exists(path):
                try:
                    src = rasterio.open(path)
                    src_files.append(src)
                except Exception as e:
                    logging.warning(f"[{partition_id}]   æ‰“å¼€ {path} ç”¨äºé•¶åµŒå¤±è´¥: {e}")
        
        if not src_files:
            logging.warning(f"[{partition_id}]   æ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶ç”¨äºé•¶åµŒ {date_str}_{polarization}_{orbit_state}")
            return None
        
        # æ‰§è¡Œé•¶åµŒæ“ä½œ
        logging.info(f"[{partition_id}]   é•¶åµŒ {len(src_files)} ä¸ª {polarization} æ–‡ä»¶ ({date_str}_{orbit_state})")
        mosaic_data, out_transform = merge(src_files, nodata=0)
        
        # å…³é—­æ‰€æœ‰æºæ–‡ä»¶
        for src in src_files:
            src.close()
        
        # æ£€æŸ¥é•¶åµŒæ•°æ®ç»“æ„
        if mosaic_data.shape[0] < 1:
            logging.error(f"[{partition_id}]   é•¶åµŒæ•°æ®ç»“æ„ä¸æ­£ç¡® ({date_str}_{polarization}_{orbit_state})")
            return None
        
        # åˆ›å»ºå…ƒæ•°æ®
        metadata = {
            "band_desc": f"{polarization} polarization, amplitude to dB, +50 offset, scale=200",
            "TIFFTAG_DATETIME": datetime.datetime.now().strftime("%Y:%m:%d %H:%M:%S"),
            "ORBIT_STATE": orbit_state,
            "DATE_ACQUIRED": date_str,
            "POLARIZATION": polarization,
            "MOSAIC_SOURCE_COUNT": len(src_files),
            "DESCRIPTION": f"Mosaicked Sentinel-1 SAR data ({polarization}). Values are amplitude converted to dB, shifted by +50, scaled by 200."
        }
        
        # å†™å‡ºé•¶åµŒTIFF
        write_tiff(mosaic_data[0], output_path, tpl, "int16", metadata)
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        if not validate_tiff(output_path, (tpl["height"], tpl["width"]), tpl["crs"], tpl["transform"]):
            logging.error(f"[{partition_id}]   âœ— é•¶åµŒTIFFéªŒè¯å¤±è´¥ ({date_str}_{polarization}_{orbit_state})")
            if Path(output_path).exists():
                Path(output_path).unlink()
            return None
        
        # è®°å½•æˆåŠŸå®Œæˆå’Œæ–‡ä»¶å¤§å°
        file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
        logging.info(f"[{partition_id}]   âœ“ æˆåŠŸåˆ›å»ºé•¶åµŒ {output_path} ({file_size_mb:.2f} MB)")
        
        return output_path
    
    except Exception as e:
        logging.error(f"[{partition_id}]   âœ— åˆ›å»ºé•¶åµŒ {date_str}_{polarization}_{orbit_state} æ—¶å‡ºé”™: {e}")
        return None

# â”€â”€â”€ å¤„ç†å•æ—¥è§‚æµ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_day_orbit(key, items, tpl, bbox_proj, mask_np, out_dir, resolution, chunksize, min_coverage, partition_id, overwrite=False):
    """å¤„ç†åŒä¸€æ—¥æœŸå’Œè½¨é“çŠ¶æ€çš„æ‰€æœ‰itemsï¼ŒåŒ…å«20åˆ†é’Ÿè¶…æ—¶æ§åˆ¶"""
    date_str, orbit_state = key.split("_")
    logging.info(f"[{partition_id}] â†’ {key} (item={len(items)})")
    t0 = time.time()
    
    try:
        # ä½¿ç”¨20åˆ†é’Ÿè¶…æ—¶æ§åˆ¶å•æ—¥å¤„ç†
        with timeout_handler(DAY_TIMEOUT):
            # ç¡®ä¿è¾“å‡ºç›®å½•æ˜¯Pathå¯¹è±¡
            out_dir = Path(out_dir)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            out_dir.mkdir(parents=True, exist_ok=True)
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ (ç›´æ¥è¾“å‡ºåˆ°ç›®æ ‡ç›®å½•ï¼Œä¸åˆ›å»ºå­æ–‡ä»¶å¤¹)
            vv_out = out_dir / f"{date_str}_vv_{orbit_state}.tiff"
            vh_out = out_dir / f"{date_str}_vh_{orbit_state}.tiff"
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”ä¸è¦†ç›–ï¼Œåˆ™è·³è¿‡
            if not overwrite and vv_out.exists() and vh_out.exists():
                # éªŒè¯ç°æœ‰æ–‡ä»¶
                vv_valid = validate_tiff(vv_out, (tpl["height"], tpl["width"]), tpl["crs"], tpl["transform"])
                vh_valid = validate_tiff(vh_out, (tpl["height"], tpl["width"]), tpl["crs"], tpl["transform"])
                
                if vv_valid and vh_valid:
                    logging.info(f"[{partition_id}]   å·²å­˜åœ¨æœ‰æ•ˆæ–‡ä»¶ï¼Œè·³è¿‡")
                    return True
                else:
                    logging.warning(f"[{partition_id}]   æ–‡ä»¶å­˜åœ¨ä½†éªŒè¯å¤±è´¥ï¼Œé‡æ–°å¤„ç†")
                    # åˆ é™¤æ— æ•ˆæ–‡ä»¶
                    if not vv_valid and vv_out.exists():
                        vv_out.unlink()
                    if not vh_valid and vh_out.exists():
                        vh_out.unlink()
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = tempfile.mkdtemp(prefix=f"s1_{date_str}_{orbit_state}_")
            logging.debug(f"[{partition_id}]   ä¸´æ—¶ç›®å½•: {temp_dir}")
            
            try:
                # å¤„ç†æ¯ä¸ªitemï¼Œå…è®¸éƒ¨åˆ†å¤±è´¥
                vv_temp_files = []
                vh_temp_files = []
                processed_count = 0  # æˆåŠŸå¤„ç†çš„æ•°é‡
                failed_count = 0     # çœŸæ­£å¤±è´¥çš„æ•°é‡
                skipped_count = 0    # è·³è¿‡çš„æ•°é‡
                
                for i, item in enumerate(items):
                    item_start_time = time.time()
                    logging.info(f"[{partition_id}]   å¤„ç†item {i+1}/{len(items)}")
                    
                    vv_path, vh_path, status = process_item(item, tpl, bbox_proj, mask_np, resolution, chunksize, temp_dir, min_coverage, partition_id)
                    
                    if status == "success":
                        processed_count += 1
                        if vv_path:
                            vv_temp_files.append(str(vv_path))
                        if vh_path:
                            vh_temp_files.append(str(vh_path))
                        
                        item_duration = time.time() - item_start_time
                        logging.info(f"[{partition_id}]   item {i+1} å¤„ç†æˆåŠŸï¼Œç”¨æ—¶ {item_duration:.1f}s")
                    elif status == "skipped":
                        skipped_count += 1
                        item_duration = time.time() - item_start_time
                        logging.info(f"[{partition_id}]   item {i+1} è·³è¿‡ï¼ˆè¦†ç›–ç‡ä¸è¶³æˆ–ç¼ºå°‘æ³¢æ®µï¼‰ï¼Œç”¨æ—¶ {item_duration:.1f}s")
                    else:  # status == "failed"
                        failed_count += 1
                        item_duration = time.time() - item_start_time
                        logging.warning(f"[{partition_id}]   item {i+1} å¤„ç†å¤±è´¥ï¼Œç”¨æ—¶ {item_duration:.1f}s")
                
                # è®°å½•å¤„ç†ç»Ÿè®¡
                logging.info(f"[{partition_id}]   itemså¤„ç†ç»Ÿè®¡: æˆåŠŸ {processed_count}, è·³è¿‡ {skipped_count}, å¤±è´¥ {failed_count} (æ€»å…± {len(items)})")
                
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶ï¼Œæ ¹æ®åŸå› ç»™å‡ºä¸åŒçš„æ¶ˆæ¯
                if not vv_temp_files and not vh_temp_files:
                    if processed_count == 0 and skipped_count > 0:
                        logging.info(f"[{partition_id}]   {key} æ‰€æœ‰itemså‡å› è¦†ç›–ç‡ä¸è¶³æˆ–ç¼ºå°‘æ³¢æ®µè€Œè·³è¿‡")
                        return True  # è·³è¿‡ä¸ç®—å¤±è´¥
                    else:
                        logging.warning(f"[{partition_id}]   æ²¡æœ‰ä¸º {key} ç”Ÿæˆä»»ä½•æœ‰æ•ˆæ–‡ä»¶")
                        return False
                
                # å¤„ç†VVæ–‡ä»¶
                vv_success = False
                if vv_temp_files:
                    # å¦‚æœåªæœ‰ä¸€ä¸ªVVæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
                    if len(vv_temp_files) == 1:
                        logging.info(f"[{partition_id}]   åªæœ‰ä¸€ä¸ªæœ‰æ•ˆçš„VVæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨")
                        shutil.copy2(vv_temp_files[0], vv_out)
                        vv_success = True
                    # å¤šä¸ªVVæ–‡ä»¶éœ€è¦é•¶åµŒ
                    else:
                        vv_mosaic = mosaic_tiffs(vv_temp_files, vv_out, tpl, date_str, orbit_state, "VV", partition_id)
                        vv_success = vv_mosaic is not None
                
                # å¤„ç†VHæ–‡ä»¶
                vh_success = False
                if vh_temp_files:
                    # å¦‚æœåªæœ‰ä¸€ä¸ªVHæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
                    if len(vh_temp_files) == 1:
                        logging.info(f"[{partition_id}]   åªæœ‰ä¸€ä¸ªæœ‰æ•ˆçš„VHæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨")
                        shutil.copy2(vh_temp_files[0], vh_out)
                        vh_success = True
                    # å¤šä¸ªVHæ–‡ä»¶éœ€è¦é•¶åµŒ
                    else:
                        vh_mosaic = mosaic_tiffs(vh_temp_files, vh_out, tpl, date_str, orbit_state, "VH", partition_id)
                        vh_success = vh_mosaic is not None
                
                # è¾“å‡ºå¤„ç†ç»“æœ
                total_duration = time.time() - t0
                if vv_success and vh_success:
                    logging.info(f"[{partition_id}] â† {key} æˆåŠŸå¤„ç†VVå’ŒVHï¼Œç”¨æ—¶ {total_duration:.1f}s")
                    return True
                elif vv_success:
                    logging.info(f"[{partition_id}] â† {key} åªæˆåŠŸå¤„ç†VVï¼Œç”¨æ—¶ {total_duration:.1f}s")
                    return True
                elif vh_success:
                    logging.info(f"[{partition_id}] â† {key} åªæˆåŠŸå¤„ç†VHï¼Œç”¨æ—¶ {total_duration:.1f}s")
                    return True
                else:
                    logging.error(f"[{partition_id}] â† {key} å¤„ç†å¤±è´¥ï¼Œç”¨æ—¶ {total_duration:.1f}s")
                    return False
                    
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    shutil.rmtree(temp_dir)
                    logging.debug(f"[{partition_id}]   å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
                except Exception as e:
                    logging.warning(f"[{partition_id}]   æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
                    
    except TimeoutException as e:
        total_duration = time.time() - t0
        logging.error(f"[{partition_id}] â€¼ï¸  {key} å¤„ç†è¶…æ—¶ ({total_duration:.1f}s > {DAY_TIMEOUT}s): {e}")
        return False
    except Exception as e:
        total_duration = time.time() - t0
        logging.error(f"[{partition_id}] âœ— å¤„ç† {key} å‡ºé”™ (ç”¨æ—¶ {total_duration:.1f}s): {type(e).__name__} - {e}")
        return False

# â”€â”€â”€ ä¸»ç¨‹åº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    args = get_args()
    out_dir = Path(args.output).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    setup_logging(args.debug, out_dir, args.partition_id)
    logging.info(f"[{args.partition_id}] âš¡ S1 Fast Processor å¯åŠ¨ (çµæ´»å¹¶è¡Œç‰ˆæœ¬)"); 
    log_sys(args.partition_id)
    logging.info(f"[{args.partition_id}] å¤„ç†è¶…æ—¶è®¾ç½®: æ€»ä½“{PROCESS_TIMEOUT//60}åˆ†é’Ÿ, å•æ—¥{DAY_TIMEOUT//60}åˆ†é’Ÿ, å•item{ITEM_TIMEOUT//60}åˆ†é’Ÿ")
    logging.info(f"[{args.partition_id}] å¤„ç†æ—¶é—´æ®µ: {args.start_date} â†’ {args.end_date}")

    tpl, bbox_proj, bbox_ll, mask_np = load_roi(Path(args.input_tiff), args.partition_id)
    
    # åŸºäºè½¨é“çŠ¶æ€æœç´¢items
    if args.orbit_state == "both":
        logging.info(f"[{args.partition_id}] æœç´¢ä¸Šå‡å’Œä¸‹é™è½¨é“æ•°æ®")
        items = search_items(bbox_ll, f"{args.start_date}/{args.end_date}", partition_id=args.partition_id)
    else:
        logging.info(f"[{args.partition_id}] æœç´¢ {args.orbit_state} è½¨é“æ•°æ®")
        items = search_items(bbox_ll, f"{args.start_date}/{args.end_date}", args.orbit_state, args.partition_id)
    
    if not items:
        logging.warning(f"[{args.partition_id}] æ— æ»¡è¶³æ¡ä»¶çš„å½±åƒï¼Œé€€å‡º")
        return

    # æŒ‰æ—¥æœŸå’Œè½¨é“çŠ¶æ€åˆ†ç»„
    groups = group_by_date_orbit(items, args.partition_id)

    with make_client(args.dask_workers, args.worker_memory, args.partition_id):
        report_path = out_dir / f"dask-report-{args.partition_id}.html"
        with performance_report(filename=report_path):
            # åˆ›å»ºçº¿ç¨‹æ± å¤„ç†å¤šä¸ªæ—¥æœŸ
            with concurrent.futures.ThreadPoolExecutor(max_workers=args.workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_group = {}
                for key, group_items in groups.items():
                    future = executor.submit(
                        process_day_orbit, key, group_items, tpl, bbox_proj, mask_np,
                        str(out_dir), SAR_RESOLUTION, args.chunksize, args.min_coverage,
                        args.partition_id, args.overwrite
                    )
                    future_to_group[future] = key
                
                # å¤„ç†ç»“æœ
                results = []
                for future in concurrent.futures.as_completed(future_to_group):
                    key = future_to_group[future]
                    try:
                        success = future.result()
                        results.append(success)
                    except Exception as e:
                        logging.error(f"[{args.partition_id}] å¤„ç† {key} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                        results.append(False)
    
    success_count = sum(results)
    total_count = len(results)
    
    logging.info(f"[{args.partition_id}] âœ… åˆ†åŒºå¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{total_count} å¤©")
    logging.info(f"[{args.partition_id}] ğŸ“Š Dask æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    if success_count == 0:
        sys.exit(1)  # å…¨éƒ¨å¤±è´¥
    elif success_count < total_count:
        logging.warning(f"[{args.partition_id}] âš ï¸  éƒ¨åˆ†æ—¥æœŸå¤„ç†å¤±è´¥ ({total_count - success_count}/{total_count})")
        sys.exit(2)  # éƒ¨åˆ†å¤±è´¥
    else:
        sys.exit(0)  # å…¨éƒ¨æˆåŠŸ

if __name__ == "__main__":
    main()